{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- **Loading text**\n",
    "- **Text preprocessing**\n",
    "- **Text vectorization**\n",
    "- **Train and test splitting**\n",
    "- **Training a classification model**\n",
    "    - Logistic Regression Classifier\n",
    "    - Naive Bayes Classifier\n",
    "    - SVM Classifier\n",
    "    - KNN Classifier\n",
    "    - Decision Tree classifier\n",
    "    - MLP Classifier\n",
    "- **Exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will illustrate the main functionalities of scikit-learn when dealing with texts.\n",
    "We will go through a typical text mining pipeline where our goal is to train a machine learning model that classifies texts scraped from company website into two classes:\n",
    "\n",
    "* 0: Text from a non-software company website.\n",
    "* 1: Text from a software company website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading text\n",
    "\n",
    "We will use the Pandas `pd.read_csv()` function to read in the textfiles containing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:40.591039Z",
     "start_time": "2020-06-26T10:47:40.469186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>software</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://autzen-reimers.de</td>\n",
       "      <td>Seite: « 1 / 0 » « 1 / 0 »AUTZEN &amp; REIMERSARCH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ibos-goerlitz.de/</td>\n",
       "      <td>Das Ingenieurbüro IBOS GmbH wurde am 17.09.199...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://kaizhong-vogt.de/</td>\n",
       "      <td>capanne.gittinger.de</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://baecker-holland.de/</td>\n",
       "      <td>Klicken Sie hier um zu unserem Kon­takt­for­mu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.vbhnr.de/privatkunden.html</td>\n",
       "      <td>Um Ihnen eine bessere Nutzung unserer Seite zu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       url  \\\n",
       "ID                                           \n",
       "0                 http://autzen-reimers.de   \n",
       "1                https://ibos-goerlitz.de/   \n",
       "2                https://kaizhong-vogt.de/   \n",
       "3              https://baecker-holland.de/   \n",
       "4   https://www.vbhnr.de/privatkunden.html   \n",
       "\n",
       "                                                 text  software  \n",
       "ID                                                               \n",
       "0   Seite: « 1 / 0 » « 1 / 0 »AUTZEN & REIMERSARCH...         0  \n",
       "1   Das Ingenieurbüro IBOS GmbH wurde am 17.09.199...         0  \n",
       "2                                capanne.gittinger.de         0  \n",
       "3   Klicken Sie hier um zu unserem Kon­takt­for­mu...         0  \n",
       "4   Um Ihnen eine bessere Nutzung unserer Seite zu...         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data = pd.read_csv(\"data/labelled_data.txt\", sep=\"\\t\", encoding=\"utf-8\", error_bad_lines=False,index_col = 0)\n",
    "labelled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our data is in a table format with 4 columns:\n",
    "\n",
    "* **ID**: unique identifiers for each observation\n",
    "* **url**: the website address from where text was downloaded\n",
    "* **text**: the downloaded website text\n",
    "* **software**: the label which tells us whether a website is from a software company (\"1\") or not (\"0\")\n",
    "\n",
    "Using Pandas, we can perform some preliminary **data exploration** to check the characteristics of our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:40.607233Z",
     "start_time": "2020-06-26T10:47:40.595139Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents: 2000\n",
      "number of columns: 3\n",
      "number of 'software' documents: 284\n",
      "number of 'non-software' documents: 1716\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_cols = labelled_data.shape\n",
    "n_software = labelled_data[\"software\"].sum()\n",
    "\n",
    "print(\"number of documents:\", n_samples)\n",
    "print(\"number of columns:\", n_cols)\n",
    "print(\"number of 'software' documents:\", n_software)\n",
    "print(\"number of 'non-software' documents:\", n_samples - n_software)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can use `value_counts()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1716\n",
       "1     284\n",
       "Name: software, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " labelled_data[\"software\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the statistics of the length of our pieces of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:40.642997Z",
     "start_time": "2020-06-26T10:47:40.613390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      2000.000000\n",
       "mean       2558.813500\n",
       "std        5144.796468\n",
       "min           1.000000\n",
       "25%         703.000000\n",
       "50%        1472.500000\n",
       "75%        2788.500000\n",
       "max      135504.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data[\"text\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily plot the distribution of website text lengths by using pandas `plot()` method and passing the keywords for a histogram with 100 bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:41.201888Z",
     "start_time": "2020-06-26T10:47:40.656679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x19375adf130>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labelled_data[\"text\"].apply(len).plot(kind=\"hist\", bins=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print some website text, just to get the feeling of the characteristics of the texts we must deal with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:41.220863Z",
     "start_time": "2020-06-26T10:47:41.205798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seite: « 1 / 0 » « 1 / 0 »AUTZEN & REIMERSARCHITEKTEN \n",
      "\n",
      "Das Ingenieurbüro IBOS GmbH wurde am 17.09.1990 in Görlitz gegründet. Unser Handlungsschwerpunkt liegt auf dem Gebiet der Siedlungswasserwirtschaft, des kommunalen Tiefbaues, der kommunalen Infrastruktur, des Wasserbaues, der Tragwerksplanung sowie der Bauleitungsplanung und Landschaftsplanung. In einem erfahrenen Team erstellen wir Planungen in allen Leistungsphasen der HOAI inklusive der Örtlichen Bauüberwachung und Sicherheits- und Gesundheitsschutzkoordination sowie Studien, Konzeptionen, Wirtschaftlichkeitsbetrachtungen und strategische Planungen. Als unabhängiges Ingenieurbüro sind wir frei von Interessen Dritter. Gern sind wir auch für Sie tätig – wir freuen uns über Ihre ! Wir suchen einen… Ihre Bewerbung richten Sie bitte an die unter aufgeführte Adresse, gern auch per E-Mail. Mitgeschickte Dateien nur im PDF-Format (keine ZIP-Datei). Am 20.09.2018 fand unsere im Gäste- und Tagungshaus KOMENSKÝ in Herrnhut statt. 100%iges Tochterunternehmen der IBOS GmbH GörlitzEntsprechend den Vorstellungen, Wünschen und Anforderungen unserer Auftraggeber sind wir als Generalplaner in der Lage, anzubieten. © 2018Herzlich Willkommen! Hinweis zu Bewerbungen per E-Mail:Veranstaltung 2018 Leistungen KOGISAktuelle Stellenangebote \n",
      "\n",
      "capanne.gittinger.de \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(labelled_data[\"text\"][0], '\\n')\n",
    "print(labelled_data[\"text\"][1], '\\n')\n",
    "print(labelled_data[\"text\"][2], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preliminary data exploration revealed some characteristics of the text, for example:\n",
    "\n",
    "* We also may want to standardise all characters to lowercase, such that \"Software\" and \"software\" are recognized as the same words.\n",
    "* There are quiet some websites with texts that are smaller than 500 characters and some even had only a single character in their \"text\" column.\n",
    "* There are quite a lot of special characters (e.g. \"€\" or \"§\") and numbers in the text\n",
    "\n",
    "Before feeding textual data to machine learning algorithms, it is often the case to preprocess the texts to remove unwanted documents, and to *normalize* the texts by removing clear noisy content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert all characters to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:41.320641Z",
     "start_time": "2020-06-26T10:47:41.230418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "0    seite: « 1 / 0 » « 1 / 0 »autzen & reimersarch...\n",
       "1    das ingenieurbüro ibos gmbh wurde am 17.09.199...\n",
       "2                                 capanne.gittinger.de\n",
       "3    klicken sie hier um zu unserem kon­takt­for­mu...\n",
       "4    um ihnen eine bessere nutzung unserer seite zu...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data['text'] = labelled_data[\"text\"].apply(lambda text: str(text).lower())\n",
    "labelled_data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the documents with less than 500 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:41.346330Z",
     "start_time": "2020-06-26T10:47:41.326896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1649, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data = labelled_data[labelled_data[\"text\"].apply(len) > 499]\n",
    "labelled_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exclude special characters from the further analysis. We will import a python's \"regular expression\" operations and apply its `sub(\"FILTER\", \"REPLACE_STRING\")` function to the text column of our labelled dataset. We submit the `sub()` function with a regular expression telling it to delete all characters in the text that are not part of this list of characters: \"`abcdefghijklmnopqrstuvwxyzäöüß& `\" (note the final whitespace char!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:41.531364Z",
     "start_time": "2020-06-26T10:47:41.350018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>software</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ibos-goerlitz.de/</td>\n",
       "      <td>das ingenieurbüro ibos gmbh wurde am  in görli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://baecker-holland.de/</td>\n",
       "      <td>klicken sie hier um zu unserem kontaktformular...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.vbhnr.de/privatkunden.html</td>\n",
       "      <td>um ihnen eine bessere nutzung unserer seite zu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.whg-ebw.de/</td>\n",
       "      <td>informieren sie sich über die wohngebiete der ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.wittich.de/</td>\n",
       "      <td>amts und mitteilungsblätter reisemagazine maga...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       url  \\\n",
       "ID                                           \n",
       "1                https://ibos-goerlitz.de/   \n",
       "3              https://baecker-holland.de/   \n",
       "4   https://www.vbhnr.de/privatkunden.html   \n",
       "5                  https://www.whg-ebw.de/   \n",
       "6                  https://www.wittich.de/   \n",
       "\n",
       "                                                 text  software  \n",
       "ID                                                               \n",
       "1   das ingenieurbüro ibos gmbh wurde am  in görli...         0  \n",
       "3   klicken sie hier um zu unserem kontaktformular...         0  \n",
       "4   um ihnen eine bessere nutzung unserer seite zu...         0  \n",
       "5   informieren sie sich über die wohngebiete der ...         0  \n",
       "6   amts und mitteilungsblätter reisemagazine maga...         0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "labelled_data[\"text\"] = labelled_data[\"text\"].apply(lambda text: re.sub(\"[^abcdefghijklmnopqrstuvwxyzäöüß& ']\", \"\", str(text)))\n",
    "labelled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorization\n",
    "\n",
    "The machine learning algorithms we will use require us to give numerical data to them. Raw text data as an input will not work! This means that we have to transfer our texts to some kind of numerical representation without loosing too much information. Transferring a text from a sequence of characters to a vector of numbers is called **text vectorization**.\n",
    "\n",
    "![text_vectorization.png](images/text_vectorization.png)\n",
    "\n",
    "There are many different ways to vectorize texts, from fancy techniques like [word embeddings](https://en.wikipedia.org/wiki/Word_embedding) and topic models like [latent dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA) to simple [bag-of-words models](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "The most intuitive way to turn the text content into numerical feature vectors is the **bag of words representation**:\n",
    "\n",
    "* assign a **fixed integer id** to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).\n",
    "\n",
    "* for each document $d_i$, count the **number of occurrences** of each word $w$ and store it in $X[i, j]$ as the value of feature $w_j$ where $j$ is the index of word $w$ in the dictionary.\n",
    "\n",
    "The bag of words representation implies that `n_features` is the number of distinct words in the corpus: this number is typically larger that 100,000.\n",
    "\n",
    "If `n_samples` is 10,000, storing `X` as a NumPy array of type `float32` would require $10,000 \\times 100,000 \\times 4$ bytes = 4 GB in RAM which is barely manageable on today’s computers.\n",
    "\n",
    "Fortunately, most values in `X` will be zeros since for a given document less than a couple thousands of distinct words will be used. For this reason we say that bag of words are typically *high-dimensional sparse datasets*. We can save a lot of memory by *only storing the non-zero parts of the feature vectors in memory*.\n",
    "The `scipy.sparse` matrices are data structures that do exactly this, and scikit-learn has built-in support for these structures.\n",
    "\n",
    "scikit-learn offers a provides basic tools to process text using the bag of words representation. To build such a representation we will proceed as follows:\n",
    "\n",
    "* *tokenize* strings and *give an integer id* for each possible token, for instance by using whitespaces and punctuation as token separators.\n",
    "* *count* the occurrences of tokens in each document.\n",
    "* *normalize* and weighting with diminishing importance tokens that occur in the majority of samples (i.e. documents).\n",
    "\n",
    "![tfidf_vectorization.png](images/tfidf_vectorization.png)\n",
    "\n",
    "This approach is called [**TFIDF**](http://en.wikipedia.org/wiki/Tf–idf):\n",
    "\n",
    "* **term frequency** (TF): counts the number of times a term $t$ (word) appears in a document $d$ adjusted by the length of the document (number of all words $t'$ in document $d$).\n",
    "* **inverse document frequency** (IDF): counts the number of documents $n_t$ an individual term $t$ appears over all documents $N$.\n",
    "* **term frequency-inverse document frequency** (TFIDF): weights down common words like \"the\" and gives more weight to rare words like \"software\".\n",
    "\n",
    "To perform this vectorization, scikit-learn provides the `TfidfVectorizer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:42.742026Z",
     "start_time": "2020-06-26T10:47:41.538143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 64619)\t0.7674788380733668\n",
      "  (0, 34005)\t0.22768716559203767\n",
      "  (0, 16620)\t0.2383998438323033\n",
      "  (0, 14515)\t0.5498184265691286\n",
      "\n",
      "  (0, 34005)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer().fit(labelled_data['text'])\n",
    "print(tfidf_vect.transform(['dies ist ein test']))\n",
    "print()\n",
    "print(tfidf_vect.transform(['dast ist nacth']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output you see is a so-called *sparse matrix*. In a sparse matrix, only non-zero elements are memorized and mapped using indexes. This actually saves A LOT of memory. In the example above, there are only five non-zero elements in the matrix and their coordinates/indexes are given in the left parantheses. The elements on the right hand side give you the corresponding count value for the word mapped by the coordinates.\n",
    "\n",
    "The `fit()` and `transform()` methods can be invoked sequentially or with the `fit_transform()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:43.876353Z",
     "start_time": "2020-06-26T10:47:42.745923Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vect.fit_transform(labelled_data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the vocabulary the vectorizer learned from our data. We call the `vocabulary_` property on the trained vectorizer to retrieve the full vocabulary and the use a little loop to print the first items in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:43.906498Z",
     "start_time": "2020-06-26T10:47:43.879177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('das', 13209)\n",
      "('ingenieurbüro', 32913)\n",
      "('ibos', 31714)\n",
      "('gmbh', 27194)\n",
      "('wurde', 74328)\n",
      "('am', 1890)\n",
      "('in', 32182)\n",
      "('görlitz', 28454)\n",
      "('gegründet', 24982)\n",
      "('unser', 67469)\n",
      "('handlungsschwerpunkt', 28901)\n",
      "The vocabulary contains 76713 terms in total\n"
     ]
    }
   ],
   "source": [
    "vocabulary = tfidf_vect.vocabulary_\n",
    "\n",
    "#little loop to print the first items in the vocabulary\n",
    "for count, item in enumerate(iter(vocabulary.items())):\n",
    "    print(item)\n",
    "    if count >= 10:\n",
    "        break\n",
    "        \n",
    "print(f'The vocabulary contains {len(vocabulary)} terms in total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34005, None, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['ist'], vocabulary.get('dast'),vocabulary.get('nacth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are quiet a lot of words. It may be a good idea to shrink down our vocabulary a bit, especially because this will reduce both memory consumption and required computational power.\n",
    "\n",
    "A common approach to do so is to apply so-called *popularity-based filering*. Hereby, we exclude very common and/or extremly uncommon words from our vocabulary. This can be achieved by passing the corresponding parameters to the vectorizer during training.\n",
    "\n",
    "Let's overwrite our vectorizer and create a new one which includes only the words appearing in at lead 1% of the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:45.219836Z",
     "start_time": "2020-06-26T10:47:43.916515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 2628 terms in total\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.01)\n",
    "trained_vectorizer = vectorizer.fit(labelled_data[\"text\"])\n",
    "vocabulary = trained_vectorizer.vocabulary_\n",
    "print(f'The vocabulary contains {len(vocabulary)} terms in total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first randomly shuffle the samples in the DataFrame, then we extract the features and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:47:45.234182Z",
     "start_time": "2020-06-26T10:47:45.224464Z"
    }
   },
   "outputs": [],
   "source": [
    "# The sample method returns a random sample without replacement of items in the DataFrame\n",
    "labelled_data = labelled_data.sample(frac=1.0, random_state=12)\n",
    "\n",
    "input_texts = labelled_data[\"text\"]\n",
    "labels = labelled_data[\"software\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the features and labels into a train and test sets. The train set will contain the 75% of the whole dataset, while the test set the remaining 25%.\n",
    "\n",
    "We then train the vectorizer **on the train data set only**, and use it to compute the features for both the train and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:50:34.602789Z",
     "start_time": "2020-06-26T10:50:32.571760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1236,)\n",
      "(413,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_texts_trainset, \\\n",
    "input_texts_testset, \\\n",
    "labels_trainset, \\\n",
    "labels_testset = train_test_split(input_texts, labels, train_size=0.75)\n",
    "\n",
    "print(input_texts_trainset.shape)\n",
    "print(input_texts_testset.shape)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.01)\n",
    "trained_vectorizer = vectorizer.fit(input_texts_trainset)\n",
    "\n",
    "features_trainset = trained_vectorizer.transform(input_texts_trainset)\n",
    "features_testset = trained_vectorizer.transform(input_texts_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1236, 2577), (413, 2577))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_trainset.shape,features_testset.shape,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a logistic regression classifier, a pretty popular model for binary outcome variables. We pass it the parameter `class_weight=\"balanced\"` because we have a pretty unbalanced dataset (one class in way more frequent than the other). The \"`balanced`\" parameter will make sure that the model will pay more attention to the infrequent class (in our case the software = 1 class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:50:35.831910Z",
     "start_time": "2020-06-26T10:50:35.760871Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit_classifier = LogisticRegression(class_weight=\"balanced\")\n",
    "trained_logit_classifier = logit_classifier.fit(features_trainset, labels_trainset)\n",
    "predicted_labels = trained_logit_classifier.predict(features_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quantify the classifier's prediction performance by generating a scikit-learn **classification report**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:50:38.517003Z",
     "start_time": "2020-06-26T10:50:38.488059Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93       345\n",
      "           1       0.63      0.75      0.68        68\n",
      "\n",
      "    accuracy                           0.89       413\n",
      "   macro avg       0.79      0.83      0.81       413\n",
      "weighted avg       0.90      0.89      0.89       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels_testset, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could summarize the report as follows:\n",
    "\n",
    "* (precision) 97% of the observations that were labeled \"0\" by the classifier actually have the true label \"0\". For label \"1\" this value is only 70%\n",
    "* (recall) 93% of the observations that have the true label \"0\" were also predicted to have the label \"0\" by the classifier. For label \"1\" this value is only 76%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:51:07.708114Z",
     "start_time": "2020-06-26T10:51:07.666385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.99      0.91       345\n",
      "           1       0.40      0.03      0.05        68\n",
      "\n",
      "    accuracy                           0.83       413\n",
      "   macro avg       0.62      0.51      0.48       413\n",
      "weighted avg       0.77      0.83      0.77       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "trained_nb_classifier = nb_classifier.fit(features_trainset, labels_trainset)\n",
    "predicted_labels = trained_nb_classifier.predict(features_testset)\n",
    "\n",
    "print(classification_report(labels_testset, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:51:12.423468Z",
     "start_time": "2020-06-26T10:51:11.078012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       345\n",
      "           1       0.76      0.37      0.50        68\n",
      "\n",
      "    accuracy                           0.88       413\n",
      "   macro avg       0.82      0.67      0.71       413\n",
      "weighted avg       0.87      0.88      0.86       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC(kernel=\"linear\")\n",
    "trained_svm_classifier = svm_classifier.fit(features_trainset, labels_trainset)\n",
    "predicted_labels = trained_svm_classifier.predict(features_testset)\n",
    "\n",
    "print(classification_report(labels_testset, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:51:14.694203Z",
     "start_time": "2020-06-26T10:51:14.561693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       345\n",
      "           1       0.62      0.35      0.45        68\n",
      "\n",
      "    accuracy                           0.86       413\n",
      "   macro avg       0.75      0.65      0.68       413\n",
      "weighted avg       0.84      0.86      0.84       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(7)\n",
    "trained_knn_classifier = knn_classifier.fit(features_trainset, labels_trainset)\n",
    "predicted_labels = trained_knn_classifier.predict(features_testset)\n",
    "\n",
    "print(classification_report(labels_testset, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:51:16.887618Z",
     "start_time": "2020-06-26T10:51:16.709278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94       345\n",
      "           1       0.77      0.49      0.59        68\n",
      "\n",
      "    accuracy                           0.89       413\n",
      "   macro avg       0.84      0.73      0.77       413\n",
      "weighted avg       0.88      0.89      0.88       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=7)\n",
    "trained_dt_classifier = dt_classifier.fit(features_trainset, labels_trainset)\n",
    "predicted_labels = trained_dt_classifier.predict(features_testset)\n",
    "\n",
    "print(classification_report(labels_testset, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T10:51:28.224890Z",
     "start_time": "2020-06-26T10:51:18.825294Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       345\n",
      "           1       0.77      0.40      0.52        68\n",
      "\n",
      "    accuracy                           0.88       413\n",
      "   macro avg       0.83      0.69      0.73       413\n",
      "weighted avg       0.87      0.88      0.87       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_classifier =  MLPClassifier(hidden_layer_sizes=(100,), alpha=0.5, max_iter=1000)\n",
    "trained_mlp_classifier = mlp_classifier.fit(features_trainset, labels_trainset)\n",
    "predicted_labels = trained_mlp_classifier.predict(features_testset)\n",
    "\n",
    "print(classification_report(labels_testset, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hint\n",
    "\n",
    "Text analysis is a typical case in which you may prefer to use `Pipelines`, e.g.\n",
    "\n",
    ">```python\n",
    ">from sklearn.pipeline import Pipeline\n",
    ">\n",
    ">#Pipeline Classifier\n",
    ">text_clf = Pipeline([\n",
    ">    ('vect', CountVectorizer()),\n",
    ">    ('tfidf', TfidfTransformer()),\n",
    ">    ('clf', MultinomialNB()),\n",
    ">])\n",
    ">\n",
    ">#Training the Pipeline Classifier\n",
    ">text_clf.fit(trainingData, twenty_train.target)  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "- Load the TripAdvisor dataset (a collection of reviews)\n",
    "- Define different classification pipelines (using sklearn.pipelines):\n",
    "    - Build three text classification models using the BOW text representation and three different classifiers, namely SVM, multi nominal Naive Bayes and a decision tree\n",
    "- Evaluate the classifiers with a 5 fold cross validation\n",
    "- Compare the results achieved by the three models on the test set, in terms of accuracy\n",
    "- Comment the results achieved in terms of precision and recall per class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from previous examples, texts are organized in folders on your disk:\n",
    "```\n",
    "|__TripAdvisor\n",
    "    |__Class1\n",
    "       |______file1\n",
    "       |______file2\n",
    "       |______...\n",
    "       |______file100\n",
    "    |__Class2\n",
    "       |______file101\n",
    "       |______file102\n",
    "       |______...\n",
    "       |______file200\n",
    "    |__ ...\n",
    "```\n",
    "\n",
    "The folder names represents the names of the classes.\n",
    "We will use the `sklearn.datasets.load_files` utility to read the files an prepare our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 499)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "# load files from directory\n",
    "categories = ['Class1', 'Class2', 'Class3', 'Class4', 'Class5']\n",
    "dataSet=load_files(\"data/TripAdvisor\",description=None,categories=categories, encoding='cp437',load_content=True, shuffle=True, random_state=42)\n",
    "\n",
    "len(dataSet.data), len(dataSet.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
